{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252421ac",
   "metadata": {},
   "source": [
    "# 多分类问题\n",
    "\n",
    "transforms是常用的图像预处理方法， 这个在torchvision计算机视觉工具包中，我们在安装Pytorch的时候顺便安装了这个torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b03843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a184f",
   "metadata": {},
   "source": [
    "## pytorch的图像预处理 transforms\n",
    "\n",
    "transform的compose可以对图像进行一系列处理，  \n",
    "\n",
    "比如ToTensor可以将图像转变为多通道的张量 28x28 -> 1x28x28  \n",
    "\n",
    "Nomalize是进行每个像素的标准化 具体的代码掠过，本次主题是另一个任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6c0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f756b9",
   "metadata": {},
   "source": [
    "## Otto Group Product Classification（重点）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be868697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#首先将类别标签转化为id\n",
    "def lables2id(lables):\n",
    "        target_id = []\n",
    "        target_lables = ['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9']\n",
    "        for lable in lables:\n",
    "            target_id.append(target_lables.index(lable))\n",
    "        return target_id\n",
    "    \n",
    "    \n",
    "# 定义数据集类 这里用pandsa去读\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self,filepath):\n",
    "        data = pd.read_csv(filepath)\n",
    "        lables = data['target']\n",
    "        self.len = data.shape[0] #多少行\n",
    "        \n",
    "        self.x_data = torch.tensor(np.array(data)[:,1:-1].astype(float))#这里突然脑抽了想了好久，切片-1，最后一列是不在里面的\n",
    "        self.y_data = lables2id(lables)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    " \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "train_dataset =ProductDataset('./train.csv')\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "#定义一个net\n",
    "\n",
    "class net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(93, 64)\n",
    "        self.linear2 = torch.nn.Linear(64, 32)\n",
    "        self.linear3 = torch.nn.Linear(32, 16)\n",
    "        self.linear4 = torch.nn.Linear(16, 9)\n",
    "        self.activate = torch.nn.ReLU()#这里\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activate(self.linear1(x))\n",
    "        x = self.activate(self.linear2(x))\n",
    "        x = self.activate(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = net()\n",
    "\n",
    "#定义损失函数和优化器\n",
    "criterion = torch.nn.CrossEntropyLoss()#用交叉熵\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5912b0a",
   "metadata": {},
   "source": [
    "这里涉及两个概念，momentun和relu激活函数。\n",
    "\n",
    "momenun真的很帅，简而言之就是确定梯度下降距离的时候要算上上一次梯度得到距离，可以看作是小球的惯性！！帅下一期我将学习并且整理所有优化器的原理  \n",
    "  \n",
    "  \n",
    "活函数来说，ReLU有以下优势：对于线性函数而言，ReLU的表达能力更强，尤其体现在深度网络中；而对于非线性函数而言，ReLU由于非负区间的梯度为常数，因此不存在梯度消失问题(Vanishing Gradient Problem)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74d76094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   300] loss: 1.046\n",
      "[1,   600] loss: 0.691\n",
      "[1,   900] loss: 0.645\n",
      "[2,   300] loss: 0.624\n",
      "[2,   600] loss: 0.602\n",
      "[2,   900] loss: 0.583\n",
      "[3,   300] loss: 0.571\n",
      "[3,   600] loss: 0.567\n",
      "[3,   900] loss: 0.556\n",
      "[4,   300] loss: 0.542\n",
      "[4,   600] loss: 0.554\n",
      "[4,   900] loss: 0.529\n",
      "[5,   300] loss: 0.522\n",
      "[5,   600] loss: 0.515\n",
      "[5,   900] loss: 0.528\n",
      "[6,   300] loss: 0.505\n",
      "[6,   600] loss: 0.510\n",
      "[6,   900] loss: 0.514\n",
      "[7,   300] loss: 0.493\n",
      "[7,   600] loss: 0.497\n",
      "[7,   900] loss: 0.507\n",
      "[8,   300] loss: 0.484\n",
      "[8,   600] loss: 0.495\n",
      "[8,   900] loss: 0.484\n",
      "[9,   300] loss: 0.472\n",
      "[9,   600] loss: 0.491\n",
      "[9,   900] loss: 0.484\n",
      "[10,   300] loss: 0.463\n",
      "[10,   600] loss: 0.472\n",
      "[10,   900] loss: 0.480\n",
      "[11,   300] loss: 0.463\n",
      "[11,   600] loss: 0.461\n",
      "[11,   900] loss: 0.475\n",
      "[12,   300] loss: 0.454\n",
      "[12,   600] loss: 0.467\n",
      "[12,   900] loss: 0.464\n",
      "[13,   300] loss: 0.448\n",
      "[13,   600] loss: 0.450\n",
      "[13,   900] loss: 0.472\n",
      "[14,   300] loss: 0.446\n",
      "[14,   600] loss: 0.442\n",
      "[14,   900] loss: 0.460\n",
      "[15,   300] loss: 0.448\n",
      "[15,   600] loss: 0.448\n",
      "[15,   900] loss: 0.440\n",
      "[16,   300] loss: 0.435\n",
      "[16,   600] loss: 0.436\n",
      "[16,   900] loss: 0.454\n",
      "[17,   300] loss: 0.433\n",
      "[17,   600] loss: 0.438\n",
      "[17,   900] loss: 0.441\n",
      "[18,   300] loss: 0.420\n",
      "[18,   600] loss: 0.445\n",
      "[18,   900] loss: 0.439\n",
      "[19,   300] loss: 0.431\n",
      "[19,   600] loss: 0.423\n",
      "[19,   900] loss: 0.438\n",
      "[20,   300] loss: 0.421\n",
      "[20,   600] loss: 0.439\n",
      "[20,   900] loss: 0.431\n",
      "[21,   300] loss: 0.423\n",
      "[21,   600] loss: 0.423\n",
      "[21,   900] loss: 0.432\n",
      "[22,   300] loss: 0.411\n",
      "[22,   600] loss: 0.424\n",
      "[22,   900] loss: 0.433\n",
      "[23,   300] loss: 0.417\n",
      "[23,   600] loss: 0.419\n",
      "[23,   900] loss: 0.421\n",
      "[24,   300] loss: 0.419\n",
      "[24,   600] loss: 0.420\n",
      "[24,   900] loss: 0.415\n",
      "[25,   300] loss: 0.396\n",
      "[25,   600] loss: 0.429\n",
      "[25,   900] loss: 0.423\n",
      "[26,   300] loss: 0.406\n",
      "[26,   600] loss: 0.416\n",
      "[26,   900] loss: 0.418\n",
      "[27,   300] loss: 0.406\n",
      "[27,   600] loss: 0.413\n",
      "[27,   900] loss: 0.414\n",
      "[28,   300] loss: 0.406\n",
      "[28,   600] loss: 0.410\n",
      "[28,   900] loss: 0.413\n",
      "[29,   300] loss: 0.401\n",
      "[29,   600] loss: 0.416\n",
      "[29,   900] loss: 0.402\n",
      "[30,   300] loss: 0.401\n",
      "[30,   600] loss: 0.407\n",
      "[30,   900] loss: 0.407\n",
      "[31,   300] loss: 0.394\n",
      "[31,   600] loss: 0.410\n",
      "[31,   900] loss: 0.410\n",
      "[32,   300] loss: 0.394\n",
      "[32,   600] loss: 0.397\n",
      "[32,   900] loss: 0.411\n",
      "[33,   300] loss: 0.388\n",
      "[33,   600] loss: 0.401\n",
      "[33,   900] loss: 0.404\n",
      "[34,   300] loss: 0.388\n",
      "[34,   600] loss: 0.399\n",
      "[34,   900] loss: 0.406\n",
      "[35,   300] loss: 0.391\n",
      "[35,   600] loss: 0.401\n",
      "[35,   900] loss: 0.392\n",
      "[36,   300] loss: 0.392\n",
      "[36,   600] loss: 0.389\n",
      "[36,   900] loss: 0.399\n",
      "[37,   300] loss: 0.384\n",
      "[37,   600] loss: 0.392\n",
      "[37,   900] loss: 0.400\n",
      "[38,   300] loss: 0.381\n",
      "[38,   600] loss: 0.394\n",
      "[38,   900] loss: 0.395\n",
      "[39,   300] loss: 0.374\n",
      "[39,   600] loss: 0.395\n",
      "[39,   900] loss: 0.405\n",
      "[40,   300] loss: 0.381\n",
      "[40,   600] loss: 0.394\n",
      "[40,   900] loss: 0.389\n",
      "[41,   300] loss: 0.381\n",
      "[41,   600] loss: 0.389\n",
      "[41,   900] loss: 0.398\n",
      "[42,   300] loss: 0.375\n",
      "[42,   600] loss: 0.389\n",
      "[42,   900] loss: 0.398\n",
      "[43,   300] loss: 0.377\n",
      "[43,   600] loss: 0.385\n",
      "[43,   900] loss: 0.384\n",
      "[44,   300] loss: 0.380\n",
      "[44,   600] loss: 0.381\n",
      "[44,   900] loss: 0.389\n",
      "[45,   300] loss: 0.367\n",
      "[45,   600] loss: 0.381\n",
      "[45,   900] loss: 0.398\n",
      "[46,   300] loss: 0.368\n",
      "[46,   600] loss: 0.384\n",
      "[46,   900] loss: 0.387\n",
      "[47,   300] loss: 0.373\n",
      "[47,   600] loss: 0.381\n",
      "[47,   900] loss: 0.384\n",
      "[48,   300] loss: 0.374\n",
      "[48,   600] loss: 0.378\n",
      "[48,   900] loss: 0.385\n",
      "[49,   300] loss: 0.362\n",
      "[49,   600] loss: 0.384\n",
      "[49,   900] loss: 0.379\n",
      "[50,   300] loss: 0.367\n",
      "[50,   600] loss: 0.374\n",
      "[50,   900] loss: 0.389\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        inputs, target = data\n",
    "        inputs = inputs.float()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:  # 每300轮打印一次结果\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "#开始训练         \n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(50):\n",
    "        train(epoch)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#预测函数\n",
    "def predict_save():\n",
    "    with torch.no_grad():\n",
    "      test_data = pd.read_csv('./test.csv')\n",
    "      x_text = torch.tensor(np.array(test_data)[:, 1:].astype(float))\n",
    "      y_pred = model(x_text.float())\n",
    "      _, predicted = torch.max(y_pred, dim=1)  # 这里先取出最大概率的索引，即是所预测的类别。\n",
    "      out = pd.get_dummies(predicted)  # get_dummies 利用pandas实现one hot encode，方便保存为预测文件。\n",
    "\n",
    "      lables = ['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9']\n",
    "      # 添加列标签\n",
    "      out.columns = lables\n",
    "      # 插入id行\n",
    "      out.insert(0, 'id', test_data['id'])\n",
    "      result = pd.DataFrame(out)\n",
    "      result.to_csv('my_predict.csv', index=False)\n",
    "        \n",
    "        \n",
    "predict_save()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e988281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
